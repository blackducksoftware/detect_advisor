import os
import zipfile
from math import trunc
#import tempfile
import io, re
import argparse
import platform

#
# Constants
advisor_version = "0.4 Beta"
detect_version = "6.2.1"
srcext_list = ['.R','.actionscript','.ada','.adb','.ads','.aidl','.as','.asm','.asp',\
'.aspx','.awk','.bas','.bat','.bms','.c','.c++','.cbl','.cc','.cfc','.cfm','.cgi','.cls',\
'.cpp','.cpy','.cs','.cxx','.el','.erl','.f','.f77','.f90','.for','.fpp','.frm','.fs',\
'.g77','.g90','.go','.groovy','.h','.hh','.hpp','.hrl','.hxx','.idl','.java','.js','.jsp',\
'.jws','.l','.lisp','.lsp','.lua','.m','.m4','.mm','.pas','.php','.php3','.php4','.pl',\
'.pm','.py','.rb','.rc','.rexx','.s','.scala','.scm','.sh','.sqb','.sql','.tcl','.tk',\
'.v','.vb','.vbs','.vhd','.vhdl','.y']
binext_list = ['.dll', '.obj', '.o', '.a', '.lib', '.iso', '.qcow2', '.vmdk', '.vdi', \
'.ova', '.nbi', '.vib', '.exe', '.img', '.bin', '.apk', '.aac', '.ipa', '.msi']
arcext_list = ['.zip', '.gz', '.tar', '.xz', '.lz', '.bz2', '.7z', '.rar', '.rar', \
'.cpio', '.Z', '.lz4', '.lha', '.arj']
jarext_list = ['.jar', '.ear', '.war']
supported_zipext_list = ['.jar', '.ear', '.war', '.zip']
pkgext_list = ['.rpm', '.deb', '.dmg']
lic_list = ['LICENSE', 'LICENSE.txt', 'notice.txt', 'license.txt', 'license.html', 'NOTICE', 'NOTICE.txt']

detectors_file_dict = {
'build.env': ['bitbake'],
'compile_commands.json': ['clang'],
'Podfile.lock': ['pod'],
'environment.yml': ['conda'],
'Makefile.PL': ['cpan'],
'packrat.lock': ['rtools'],
'Gopkg.lock': ['go'],
'gogradle.lock': ['go'],
'go.mod': ['go'],
'vendor.json': ['go'],
'vendor.conf': ['go'],
'build.gradle': ['gradlew','gradle'],
'build.gradle.kts': ['gradlew','gradle'],
'rebar.config': ['rebar3'],
'pom.xml': ['mvnw','mvn'],
'pom.groovy': ['mvnw','mvn'],
'node_modules': ['npm'],
'package.json': ['npm'],
'package-lock.json': ['npm'],
'npm-shrinkwrap.json': ['npm'],
'composer.lock': ['composer'],
'composer.json': ['composer'],
'package.xml': ['pear'],
'pipfile': ['python','python3','pipenv'],
'pipfile.lock': ['python','python3','pipenv'],
'setup.py': ['python','python3','pip'],
'requirements.txt': ['python','python3','pip'],
'Gemfile.lock': ['gem'],
'build.sbt': ['sbt'],
'Package.swift': ['swift'],
'yarn.lock': ['yarn'],
'Makefile': ['clang'],
'makefile': ['clang'],
'GNUmakefile': ['clang'],
'recipe-depends.dot': ['bitbake'],
'task-depends.dot': ['bitbake']
}

detectors_ext_dict = {
'.csproj': ['dotnet'],
'.fsproj': ['dotnet'],
'.vbproj': ['dotnet'],
'.asaproj': ['dotnet'],
'.dcproj': ['dotnet'],
'.shproj': ['dotnet'],
'.ccproj': ['dotnet'],
'.sfproj': ['dotnet'],
'.njsproj': ['dotnet'],
'.vcxproj': ['dotnet'],
'.vcproj': ['dotnet'],
'.xproj': ['dotnet'],
'.pyproj': ['dotnet'],
'.hiveproj': ['dotnet'],
'.pigproj': ['dotnet'],
'.jsproj': ['dotnet'],
'.usqlproj': ['dotnet'],
'.deployproj': ['dotnet'],
'.msbuildproj': ['dotnet'],
'.sqlproj': ['dotnet'],
'.dbproj': ['dotnet'],
'.rproj': ['dotnet'],
'.sln': ['dotnet'],
'.mk': ['clang']
}

detector_cli_options_dict = {
'bazel':
"--detect.bazel.cquery.options='OPTION1,OPTION2'\n" + \
"    (OPTIONAL List of additional options to pass to the bazel cquery command.)\n" + \
"--detect.bazel.dependency.type=MAVEN_JAR/MAVEN_INSTALL/UNSPECIFIED\n" + \
"    (OPTIONAL Bazel workspace external dependency rule: The Bazel workspace rule used to pull in external dependencies.\n" + \
"    If not set, Detect will attempt to determine the rule from the contents of the WORKSPACE file (default: UNSPECIFIED).)\n",
'bitbake':
"--detect.bitbake.package.names='PACKAGE1,PACKAGE2'\n" + \
"    (OPTIONAL List of package names from which dependencies are extracted.)\n" + \
"--detect.bitbake.search.depth=X\n" + \
"    (OPTIONAL The depth at which Detect will search for the recipe-depends.dot or package-depends.dot files (default: 1).)\n" + \
"--detect.bitbake.source.arguments='ARG1,ARG2,ARG3'\n" + \
"    (OPTIONAL List of arguments to supply when sourcing the build environment init script)\n",
'clang':
"    Note that Detect supports reading a compile_commands.json file generated by cmake.\n" + \
"    If the project does not use cmake then it is possible to produce the compile_commands.json\n" + \
"    from standard make using utilities such as https://github.com/rizsotto/Bear. Detect must be\n" + \
"    run on Linux only for the detection of OSS packages using compile_commands.json, and the packages\n" + \
"    must be installed in the OS.\n",
'conda':
"--detect.conda.environment.name=NAME\n" + \
"    (OPTIONAL The name of the anaconda environment used by your project)\n",
'dotnet':
"--detect.nuget.config.path=PATH\n" + \
"    (OPTIONAL The path to the Nuget.Config file to supply to the nuget exe)\n" + \
"--detect.nuget.packages.repo.url=URL\n" + \
"    (OPTIONAL Nuget Packages Repository URL (default: https://api.nuget.org/v3/index.json).)\n" + \
"--detect.nuget.excluded.modules=PROJECT\n" + \
"    (OPTIONAL Nuget Projects Excluded: The names of the projects in a solution to exclude.)\n" + \
"--detect.nuget.ignore.failure=true\n" + \
"    (OPTIONAL Ignore Nuget Failures: If true errors will be logged and then ignored.)\n" + \
"--detect.nuget.included.modules=PROJECT\n" + \
"    (OPTIONAL Nuget Modules Included: The names of the projects in a solution to include (overrides exclude).)\n",
'gradle':
"--detect.gradle.build.command='ARGUMENT1 ARGUMENT2'\n" + \
"    (OPTIONAL Gradle Build Command: Gradle command line arguments to add to the mvn/mvnw command line.)\n" + \
"--detect.gradle.excluded.configurations='CONFIG1,CONFIG2'\n" + \
"    (OPTIONAL Gradle Exclude Configurations: List of Gradle configurations to exclude.)\n" + \
"--detect.gradle.excluded.projects='PROJECT1,PROJECT2'\n" + \
"    (OPTIONAL Gradle Exclude Projects: List of Gradle sub-projects to exclude.)\n" + \
"--detect.gradle.included.configurations='CONFIG1,CONFIG2'\n" + \
"    (OPTIONAL Gradle Include Configurations: List of Gradle configurations to include.)\n" + \
"--detect.gradle.included.projects='PROJECT1,PROJECT2'\n" + \
"    (OPTIONAL Gradle Include Projects: List of Gradle sub-projects to include.)\n",
'maven':
"--detect.maven.build.command='ARGUMENT1 ARGUMENT2'\n" + \
"    (OPTIONAL Maven Build Command: Maven command line arguments to add to the mvn/mvnw command line.)\n" + \
"--detect.maven.excluded.scopes='SCOPE1,SCOPE2'\n" + \
"    (OPTIONAL Dependency Scope Excluded: List of Maven scopes. Output will be limited to dependencies outside these scopes (overrides include).)\n" + \
"--detect.maven.included.scopes='SCOPE1,SCOPE2'\n" + \
"    (OPTIONAL Dependency Scope Included: List of Maven scopes. Output will be limited to dependencies within these scopes (overridden by exclude).)\n" + \
"--detect.maven.excluded.modules='MODULE1,MODULE2'\n" + \
"    (OPTIONAL Maven Modules Excluded: List of Maven modules (sub-projects) to exclude.)\n" + \
"--detect.maven.included.modules='MODULE1,MODULE2'\n" + \
"    (OPTIONAL Maven Modules Included: List of Maven modules (sub-projects) to include.)\n" + \
"--detect.maven.include.plugins=true\n" + \
"    (OPTIONAL Maven Include Plugins: Whether or not detect will include the plugins section when parsing a pom.xml.)\n",
'npm':
"--detect.npm.arguments='ARG1 ARG2'\n" + \
"    (OPTIONAL Additional arguments to add to the npm command line when running Detect against an NPM project.)\n" + \
"--detect.npm.include.dev.dependencies=false\n" + \
"    (OPTIONAL Include NPM Development Dependencies: Set this value to false if you would like to exclude your dev dependencies.)\n",
'packagist':
"--detect.packagist.include.dev.dependencies=false\n" + \
"    (OPTIONAL Include Packagist Development Dependencies: Set this value to false if you would like to exclude your dev requires dependencies.)\n",
'pear':
"--detect.pear.only.required.deps=true\n" + \
"    (OPTIONAL Include Only Required Pear Dependencies: Set to true if you would like to include only required packages.)\n",
'pip':
"--detect.pip.only.project.tree=true\n" + \
"    (OPTIONAL PIP Include Only Project Tree: By default, pipenv includes all dependencies found in the graph. Set to true to only\n" + \
"    include dependencies found underneath the dependency that matches the provided pip project and version name.)\n" + \
"--detect.pip.project.name=NAME\n" + \
"    (OPTIONAL PIP Project Name: The name of your PIP project, to be used if your project's name cannot be correctly inferred from its setup.py file.)\n" + \
"--detect.pip.project.version.name=VERSION\n" + \
"    (OPTIONAL PIP Project Version Name: The version of your PIP project, to be used if your project's version name\n" + \
"    cannot be correctly inferred from its setup.py file.)\n" + \
"--detect.pip.requirements.path='PATH1,PATH2'\n" + \
"    (OPTIONAL PIP Requirements Path: List of paths to requirements.txt files.)\n",
'ruby':
"--detect.ruby.include.dev.dependencies=true\n" + \
"    (OPTIONAL Ruby Development Dependencies: If set to true, development dependencies will be included when parsing *.gemspec files.)\n" + \
"--detect.ruby.include.runtime.dependencies=false\n" + \
"    (OPTIONAL Ruby Runtime Dependencies: If set to false, runtime dependencies will not be included when parsing *.gemspec files.)\n",
'sbt':
"--detect.sbt.report.search.depth\n" + \
"    (OPTIONAL SBT Report Search Depth: Depth the sbt detector will use to search for report files (default 3))\n" + \
"--detect.sbt.excluded.configurations='CONFIG'\n" + \
"    (OPTIONAL SBT Configurations Excluded: The names of the sbt configurations to exclude.)\n" + \
"--detect.sbt.included.configurations='CONFIG'\n" + \
"    (OPTIONAL SBT Configurations Included: The names of the sbt configurations to include.)\n",
'yarn':
"--detect.yarn.prod.only=true\n" + \
"    (OPTIONAL Include Yarn Production Dependencies Only: Set this to true to only scan production dependencies.)\n"
}

detector_cli_required_dict = {
'bazel':
"--detect.bazel.target='TARGET'\n" + \
"    (REQUIRED Bazel Target: The Bazel target (for example, //foo:foolib) for which dependencies are collected.)\n",
'bitbake':
"--detect.bitbake.build.env.name=NAME\n" + \
"    (REQUIRED BitBake Init Script Name: The name of the build environment init script (default: oe-init-build-env).)\n"
}

linux_only_detectors = ['clang', 'bitbake']

largesize = 5000000
hugesize = 20000000

notinarc = 0
inarc = 1
inarcunc = 1
inarccomp = 2

#
# Variables
max_arc_depth = 0

counts = {
'file' : [0,0],
'dir' : [0,0],
'arc' : [0,0],
'bin' : [0,0],
'jar' : [0,0],
'src' : [0,0],
'det' : [0,0],
'large' : [0,0],
'huge' : [0,0],
'other' : [0,0],
'lic' : [0,0],
'pkg' : [0,0]
}

sizes = {
'file' : [0,0,0],
'dir' : [0,0,0],
'arc' : [0,0,0],
'bin' : [0,0,0],
'jar' : [0,0,0],
'src' : [0,0,0],
'det' : [0,0,0],
'large' : [0,0,0],
'huge' : [0,0,0],
'other' : [0,0,0],
'pkg' : [0,0,0]
}

src_list = []
bin_list = []
bin_large_list = []
large_list = []
huge_list = []
arc_list = []
jar_list = []
other_list = []
pkg_list = []

bdignore_list = []

det_dict = {}
detectors_list = []

crc_dict = {}

dup_dir_dict = {}
dup_large_dict = {}

dir_dict = {}
large_dict = {}
arc_files_dict = {}

messages = ""
recs_msgs_dict = {
'crit': '',
'imp': '',
'info': ''
}
cli_msgs_dict = {
'reqd': '',
'proj' : '',
'scan': '',
'size': '',
'dep': '',
'lic': '',
'rep': ''
}

cli_msgs_dict['reqd'] = "--blackduck.url=https://YOURSERVER\n" + \
"--blackduck.api.token=YOURTOKEN\n"
cli_msgs_dict['proj'] = "--detect.project.name=PROJECT_NAME\n" + \
"--detect.project.version.name=VERSION_NAME\n" + \
"    (OPTIONAL Specify project and version names)\n" + \
"--detect.project.version.update=true\n" + \
"    (OPTIONAL Update project and version parameters below for existing projects)\n" + \
"--detect.project.tier=X\n" + \
"    (OPTIONAL Define project tier numeric for new project)\n" + \
"--detect.project.version.phase=ARCHIVED/DEPRECATED/DEVELOPMENT/PLANNING/PRERELEASE/RELEASED\n" + \
"    (OPTIONAL Specify project phase for new project - default DEVELOPMENT)\n" + \
"--detect.project.version.distribution=EXTERNAL/SAAS/INTERNAL/OPENSOURCE\n" + \
"    (OPTIONAL Specify version distribution for new project - default EXTERNAL)\n" + \
"--detect.project.user.groups='GROUP1,GROUP2'\n" + \
"    (OPTIONAL Define group access for project for new project)\n"
cli_msgs_dict['rep'] = "--detect.wait.for.results=true\n" + \
"    (OPTIONAL Wait for server-side analysis to complete - useful for script execution after scan)\n" + \
"--detect.cleanup=true\n" + \
"    (OPTIONAL Retain scan results in $HOME/blackduck folder)\n" + \
"--detect.policy.check.fail.on.severities='ALL,NONE,UNSPECIFIED,TRIVIAL,MINOR,MAJOR,CRITICAL,BLOCKER'\n" + \
"    (OPTIONAL Comma-separated list of policy violation severities that will cause Detect to return fail code\n" + \
"--detect.notices.report=true\n" + \
"    (OPTIONAL Generate Notices Report in text form in project directory)\n" + \
"--detect.notices.report.path=NOTICES_PATH\n" + \
"    (OPTIONAL The output directory for notices report. Default is the project directory)\n" + \
"--detect.risk.report.pdf=true\n" + \
"    (OPTIONAL Black Duck risk report in PDF form will be created in project directory)\n" + \
"--detect.risk.report.pdf.path=PDF_PATH\n" + \
"    (OPTIONAL Output directory for risk report in PDF. Default is the project directory.\n"
"--detect.report.timeout=XXX\n" + \
"    (OPTIONAL Amount of time in seconds Detect will wait for scans to finish and to generate reports (default 300).\n" + \
"    300 seconds may be sufficient, but very large scans can take up to 20 minutes (1200 seconds) or longer)\n"

def process_nested_zip(z, zippath, zipdepth, dirdepth):
	global max_arc_depth
	global messages

	zipdepth += 1
	if zipdepth > max_arc_depth:
		max_arc_depth = zipdepth

	#print("ZIP:{}:{}".format(zipdepth, zippath))
	z2_filedata =  io.BytesIO(z.read())
	try:
		with zipfile.ZipFile(z2_filedata) as nz:
			for zinfo in nz.infolist():
				dirdepth = process_zip_entry(zinfo, zippath, dirdepth)
				if os.path.splitext(zinfo.filename)[1] in supported_zipext_list:
					with nz.open(zinfo.filename) as z2:
						process_nested_zip(z2, zippath + "##" + zinfo.filename, zipdepth, dirdepth)
	except:
		messages += "WARNING: Can't open nested zip {} (Skipped)\n".format(zippath)


def process_zip_entry(zinfo, zippath, dirdepth):
	#print("ENTRY:" + zippath + "##" + zinfo.filename)
	fullpath = zippath + "##" + zinfo.filename
	odir = zinfo.filename
	dir = os.path.dirname(zinfo.filename)
	depthinzip = 0
	while dir != odir:
		depthinzip += 1
		odir = dir
		dir = os.path.dirname(dir)

	dirdepth = dirdepth + depthinzip
	tdir = zippath + "##" + os.path.dirname(zinfo.filename)
	if tdir not in dir_dict.keys():
		counts['dir'][inarc] += 1
		dir_dict[tdir] = {}
		dir_dict[tdir]['num_entries'] = 1
		dir_dict[tdir]['size'] = zinfo.file_size
		dir_dict[tdir]['depth'] = dirdepth
		dir_dict[tdir]['filenamesstring'] = zinfo.filename + ";"
	else:
		dir_dict[tdir]['num_entries'] += 1
		dir_dict[tdir]['size'] += zinfo.file_size
		dir_dict[tdir]['depth'] = dirdepth
		dir_dict[tdir]['filenamesstring'] += zinfo.filename + ";"

	arc_files_dict[fullpath] = zinfo.CRC
	checkfile(zinfo.filename, fullpath, zinfo.file_size, zinfo.compress_size, dirdepth, True)
	return dirdepth

def process_zip(zippath, zipdepth, dirdepth):
	global max_arc_depth
	global messages

	zipdepth += 1
	if zipdepth > max_arc_depth:
		max_arc_depth = zipdepth

	#print("ZIP:{}:{}".format(zipdepth, zippath))
	try:
		with zipfile.ZipFile(zippath) as z:
			for zinfo in z.infolist():
				if zinfo.is_dir():
					continue
				fullpath = zippath + "##" + zinfo.filename
				process_zip_entry(zinfo, zippath, dirdepth)
				if os.path.splitext(zinfo.filename)[1] in supported_zipext_list:
					with z.open(zinfo.filename) as z2:
						process_nested_zip(z2, fullpath, zipdepth, dirdepth)
	except:
		messages += "WARNING: Can't open zip {} (Skipped)\n".format(zippath)

def checkfile(name, path, size, size_comp, dirdepth, in_archive):
	ext = os.path.splitext(name)[1]
#	print(ext)
	if ext != ".zip":
		if not in_archive:
			counts['file'][notinarc] += 1
			sizes['file'][notinarc] += size
		else:
			counts['file'][inarc] += 1
			sizes['file'][inarcunc] += size
			sizes['file'][inarccomp] += size_comp
		if size > hugesize:
			huge_list.append(path)
			large_dict[path] = size
			if not in_archive:
				counts['huge'][notinarc] += 1
				sizes['huge'][notinarc] += size
			else:
				counts['huge'][inarc] += 1
				sizes['huge'][inarcunc] += size
				sizes['huge'][inarccomp] += size_comp
		elif size > largesize:
			large_list.append(path)
			large_dict[path] = size
			if not in_archive:
				counts['large'][notinarc] += 1
				sizes['large'][notinarc] += size
			else:
				counts['large'][inarc] += 1
				sizes['large'][inarcunc] += size
				sizes['large'][inarccomp] += size_comp

	if os.path.basename(name) in detectors_file_dict.keys() and not path.find("node_modules"):
		if not in_archive:
			det_dict[path] = dirdepth
		ftype = 'det'
	elif os.path.basename(name) in lic_list:
		other_list.append(path)
		ftype = 'other'
		counts['lic'][notinarc] += 1
	elif (ext != ""):
		if ext in detectors_ext_dict.keys():
			if not in_archive:
				det_dict[path] = dirdepth
			ftype = 'det'
		elif ext in srcext_list:
			src_list.append(path)
			ftype = 'src'
		elif ext in jarext_list:
			jar_list.append(path)
			ftype = 'jar'
		elif ext in binext_list:
			bin_list.append(path)
			if size > largesize:
				bin_large_list.append(path)
			ftype = 'bin'
		elif ext in arcext_list:
			arc_list.append(path)
			ftype = 'arc'
		elif ext in pkgext_list:
			pkg_list.append(path)
			ftype = 'pkg'
		else:
			other_list.append(path)
			ftype = 'other'
	else:
		other_list.append(path)
		ftype = 'other'
	#print("path:{} type:{}, size_comp:{}, size:{}".format(path, ftype, size_comp, size))
	if not in_archive:
		counts[ftype][notinarc] += 1
		sizes[ftype][notinarc] += size
	else:
		counts[ftype][inarc] += 1
		sizes[ftype][inarcunc] += size
		if size_comp == 0:
			sizes[ftype][inarccomp] += size
		else:
			sizes[ftype][inarccomp] += size_comp
	return(ftype)

def process_dir(path, dirdepth):
	dir_size = 0
	dir_entries = 0
	filenames_string = ""
	global messages
	global bdignore

	dir_dict[path] = {}
	dirdepth += 1

	all_bin = False
	try:
		for entry in os.scandir(path):
			dir_entries += 1
			filenames_string += entry.name + ";"
			if entry.is_dir(follow_symlinks=False):
				counts['dir'][notinarc] += 1
				dir_size += process_dir(entry.path, dirdepth)
			else:
				ftype = checkfile(entry.name, entry.path, entry.stat(follow_symlinks=False).st_size, 0, dirdepth, False)
				if ftype == 'bin':
					if dir_entries == 1:
						all_bin = True
				else:
					all_bin = False
				ext = os.path.splitext(entry.name)[1]
				if ext in supported_zipext_list:
					process_zip(entry.path, 0, dirdepth)

				dir_size += entry.stat(follow_symlinks=False).st_size
	except OSError:
		messages += "ERROR: Unable to open folder {}\n".format(path)
		return 0

	dir_dict[path]['num_entries'] = dir_entries
	dir_dict[path]['size'] = dir_size
	dir_dict[path]['depth'] = dirdepth
	dir_dict[path]['filenamesstring'] = filenames_string
	if all_bin:
		bdignore += path + "\n"
	return dir_size

def process_largefiledups(f):
	import filecmp

	if f:
		f.write("\nLARGE DUPLICATE FILES:\n")

	fcount = 0
	total_dup_size = 0
	count_dups = 0
	fitems = len(large_dict)
	for apath, asize in large_dict.items():
		fcount += 1
		if fcount % ((fitems//6) + 1) == 0:
			print(".", end="", flush=True)
		dup = False
		for cpath, csize in large_dict.items():
			if apath == cpath:
				continue
			if asize == csize:
				aext = os.path.splitext(apath)[1]
				cext = os.path.splitext(cpath)[1]
				if aext == cext:
					dup = True
				elif aext == "" and cext == "":
					dup = True
				if dup:
					if apath.find("##") > 0 or cpath.find("##") > 0:
						if apath.find("##") > 0:
							acrc = arc_files_dict[apath]
						else:
							acrc = get_crc(apath)
						if cpath.find("##") > 0:
							ccrc = arc_files_dict[cpath]
						else:
							ccrc = get_crc(cpath)
						test = (acrc == ccrc)
					else:
						test = filecmp.cmp(apath, cpath, True)

					if test and dup_large_dict.get(cpath) == None and \
					dup_dir_dict.get(os.path.dirname(apath)) == None and \
					dup_dir_dict.get(os.path.dirname(cpath)) == None:
						dup_large_dict[apath] = cpath
						total_dup_size += asize
						count_dups += 1
						if f:
							f.write("- Large Duplicate file - {}, {} (size {}MB)\n".format(apath,cpath,trunc(asize/1000000)))
	return(count_dups, total_dup_size)

def process_dirdups(f):
	count_dupdirs = 0
	size_dupdirs = 0
	dcount = 0

	tmp_dup_dir_dict = {}

	if f:
		f.write("\nLARGE DUPLICATE FOLDERS:\n")

	ditems = len(dir_dict)
	for apath, adict in dir_dict.items():
		dcount += 1
		if dcount % ((ditems//6) + 1) == 0:
			print(".", end="", flush=True)
		try:
			if adict['num_entries'] == 0 or adict['size'] < hugesize:
				continue
		except:
			continue
		dupmatch = False
		for cpath, cdict in dir_dict.items():
			if apath != cpath:
				try:
					if adict['num_entries'] == cdict['num_entries'] and adict['size'] == cdict['size'] \
					and adict['filenamesstring'] == cdict['filenamesstring']:
						if adict['depth'] <= cdict['depth']:
							keypath = apath
							valpath = cpath
						else:
							keypath = cpath
							valpath = apath

						newdup = False
						if keypath not in tmp_dup_dir_dict.keys():
							newdup = True
						elif tmp_dup_dir_dict[keypath] != valpath:
							newdup = True
						if newdup:
							tmp_dup_dir_dict[keypath] = valpath
						break
				except:
					pass

	# Now remove dupdirs with matching parent folders
	for xpath in tmp_dup_dir_dict.keys():
		ypath = tmp_dup_dir_dict[xpath]
		#print("Processing folder:" + xpath + " dup " + ypath)
		xdir = os.path.dirname(xpath)
		ydir = os.path.dirname(ypath)
		if xdir in tmp_dup_dir_dict.keys() and tmp_dup_dir_dict[xdir] == ydir:
			# parents match - ignore
			#print("Ignorning dup dir: " + xpath + " " + ypath)
			pass
		else:
			# Create dupdir entry
			#print("Adding dup dir: " + xpath + " " + ypath)
			dup_dir_dict[xpath] = ypath
			count_dupdirs += 1
			size_dupdirs += dir_dict[xpath]['size']
			if f and dir_dict[xpath]['size'] > hugesize:
				f.write("- Large Duplicate folder - {}, {} (size {}MB)\n".format(xpath,ypath, \
				trunc(dir_dict[xpath]['size']/1000000)))

	return(count_dupdirs, size_dupdirs)

def check_singlefiles(f):

	# Check for singleton js & other single files
	sfmatch = False
	sf_list = []
	for thisfile in src_list:
		ext = os.path.splitext(thisfile)[1]
		if ext == '.js':
			# get dir
			# check for .js in filenamesstring
			thisdir = dir_dict.get(os.path.dirname(thisfile))
			if thisfile.find("node_modules") > 0:
				continue
			if thisdir != None:
				all_js = True
				for filename in thisdir['filenamesstring'].split(';'):
					srcext = os.path.splitext(filename)[1]
					if srcext != '.js':
						all_js = False
				if not all_js:
					sfmatch = True
					sf_list.append(thisfile)
	if sfmatch:
		recs_msgs_dict['info'] += "- INFORMATION: {} singleton .js files found\n".format(len(sf_list)) + \
		"    Impact:  OSS components within JS files may not be detected\n" + \
		"    Action:  Consider specifying Single file matching in Signature scan\n" + \
		"             (--detect.blackduck.signature.scanner.individual.file.matching=SOURCE)\n\n"
		if cli_msgs_dict['scan'].find("individual.file.matching") < 0:
			cli_msgs_dict['scan'] += "--detect.blackduck.signature.scanner.individual.file.matching=SOURCE\n" + \
			"    (To check singleton .js files for OSS matches)\n"

		#if f:
		#	f.write("\nSINGLE JS FILES:\n")
		#	for thisfile in sf_list:
		#		f.write("- {}\n".format(thisfile))

def get_crc(myfile):
	import zlib
	buffersize = 65536

	crcvalue = 0
	try:
		with open(myfile, 'rb') as afile:
			buffr = afile.read(buffersize)
			while len(buffr) > 0:
				crcvalue = zlib.crc32(buffr, crcvalue)
				buffr = afile.read(buffersize)
	except:
		messages += "WARNING: Unable to open file {} to calculate CRC\n".format(myfile)
		return(0)
	return(crcvalue)

def print_summary(critical_only, f):
	global rep

	summary = "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n" + \
	"SUMMARY INFO:\nTotal Scan Size = {:,d} MB\n\n".format(trunc((sizes['file'][notinarc]+sizes['arc'][notinarc])/1000000)) + \
	"                         Num Outside     Size Outside      Num Inside     Size Inside     Size Inside\n" + \
	"                            Archives         Archives        Archives        Archives        Archives\n" + \
	"                                                                        (UNcompressed)    (compressed)\n" + \
	"====================  ==============   ==============   =============   =============   =============\n"

	row = "{:25} {:>10,d}    {:>10,d} MB      {:>10,d}   {:>10,d} MB   {:>10,d} MB\n"

	summary += row.format("Files (exc. Archives)", \
	counts['file'][notinarc], \
	trunc(sizes['file'][notinarc]/1000000), \
	counts['file'][inarc], \
	trunc(sizes['file'][inarcunc]/1000000), \
	trunc(sizes['file'][inarccomp]/1000000))

	summary += row.format("Archives (exc. Jars)", \
	counts['arc'][notinarc], \
	trunc(sizes['arc'][notinarc]/1000000), \
	counts['arc'][inarc], \
	trunc(sizes['arc'][inarcunc]/1000000), \
	trunc(sizes['arc'][inarccomp]/1000000))

	summary += "--------------------  --------------   --------------   -------------   -------------   -------------\n"

	summary += "{:25} {:>10,d}              N/A      {:>10,d}             N/A             N/A   \n".format("Folders", \
	counts['dir'][notinarc], \
	counts['dir'][inarc])

	summary += row.format("Source Files", \
	counts['src'][notinarc], \
	trunc(sizes['src'][notinarc]/1000000), \
	counts['src'][inarc], \
	trunc(sizes['src'][inarcunc]/1000000), \
	trunc(sizes['src'][inarccomp]/1000000))

	summary += row.format("JAR Archives", \
	counts['jar'][notinarc], \
	trunc(sizes['jar'][notinarc]/1000000), \
	counts['jar'][inarc], \
	trunc(sizes['jar'][inarcunc]/1000000), \
	trunc(sizes['jar'][inarccomp]/1000000))

	summary += row.format("Binary Files", \
	counts['bin'][notinarc], \
	trunc(sizes['bin'][notinarc]/1000000), \
	counts['bin'][inarc], \
	trunc(sizes['bin'][inarcunc]/1000000), \
	trunc(sizes['bin'][inarccomp]/1000000))

	summary += row.format("Other Files", \
	counts['other'][notinarc], \
	trunc(sizes['other'][notinarc]/1000000), \
	counts['other'][inarc], \
	trunc(sizes['other'][inarcunc]/1000000), \
	trunc(sizes['other'][inarccomp]/1000000))

	summary += row.format("Package Mgr Files", \
	counts['det'][notinarc], \
	trunc(sizes['det'][notinarc]/1000000), \
	counts['det'][inarc], \
	trunc(sizes['det'][inarcunc]/1000000), \
	trunc(sizes['det'][inarccomp]/1000000))

	summary += row.format("OS Package Files", \
	counts['pkg'][notinarc], \
	trunc(sizes['pkg'][notinarc]/1000000), \
	counts['pkg'][inarc], \
	trunc(sizes['pkg'][inarcunc]/1000000), \
	trunc(sizes['pkg'][inarccomp]/1000000))

	summary += "====================  ==============   ==============   =============   =============   =============\n"

	summary += row.format("ALL FILES (Scan size)", counts['file'][notinarc]+counts['arc'][notinarc], \
	trunc((sizes['file'][notinarc]+sizes['arc'][notinarc])/1000000), \
	counts['file'][inarc]+counts['arc'][inarc], \
	trunc((sizes['file'][inarcunc]+sizes['arc'][inarcunc])/1000000), \
	trunc((sizes['file'][inarccomp]+sizes['arc'][inarccomp])/1000000))

	summary += "====================  ==============   ==============   =============   =============   =============\n"

	summary += row.format("Large Files (>{:1d}MB)".format(trunc(largesize/1000000)), \
	counts['large'][notinarc], \
	trunc(sizes['large'][notinarc]/1000000), \
	counts['large'][inarc], \
	trunc(sizes['large'][inarcunc]/1000000), \
	trunc(sizes['large'][inarccomp]/1000000))

	summary += row.format("Huge Files (>{:2d}MB)".format(trunc(hugesize/1000000)), \
	counts['huge'][notinarc], \
	trunc(sizes['huge'][notinarc]/1000000), \
	counts['huge'][inarc], \
	trunc(sizes['huge'][inarcunc]/1000000), \
	trunc(sizes['huge'][inarccomp]/1000000))

	summary += "--------------------  --------------   --------------   -------------   -------------   -------------\n"

	summary += rep + "\n"

	if not critical_only:
		print(summary)
	if f:
		f.write(summary)

def signature_process(folder, f):
	global bdignore

	#print("SIGNATURE SCAN ANALYSIS:")

	# Find duplicates without expanding archives - to avoid processing dups
	print("- Processing folders         ", end="", flush=True)
	num_dirdups, size_dirdups = process_dirdups(f)
	print(" Done")

	print("- Processing large files     ", end="", flush=True)
	num_dups, size_dups = process_largefiledups(f)
	print(" Done")

	print("- Processing Signature Scan  .....", end="", flush=True)

	# Produce Recommendations
	if sizes['file'][notinarc]+sizes['arc'][notinarc] > 5000000000:
		recs_msgs_dict['crit'] += "- CRITICAL: Overall scan size ({:>,d} MB) is too large\n".format(trunc((sizes['file'][notinarc]+sizes['arc'][notinarc])/1000000)) + \
		"    Impact:  Scan will fail\n" + \
		"    Action:  Ignore folders or remove large files - look for .bdignore in report file\n\n"
	elif sizes['file'][notinarc]+sizes['arc'][notinarc] > 2000000000:
		recs_msgs_dict['imp'] += "- IMPORTANT: Overall scan size ({:>,d} MB) is large\n".format(trunc((sizes['file'][notinarc]+sizes['arc'][notinarc])/1000000)) + \
		"    Impact:  Will impact Capacity license usage\n" + \
		"    Action:  Ignore folders or remove large files - look for .bdignore in report file\n\n"

	if counts['file'][notinarc]+counts['file'][inarc] > 1000000:
		recs_msgs_dict['imp'] += "- IMPORTANT: Overall number of files ({:>,d}) is very large\n".format(trunc((counts['file'][notinarc]+counts['file'][inarc]))) + \
		"    Impact:  Scan time could be VERY long\n" + \
		"    Action:  Ignore folders or split project (scan sub-projects) - look for .bdignore in report file\n\n"
	elif counts['file'][notinarc]+counts['file'][inarc] > 200000:
		recs_msgs_dict['info'] += "- INFORMATION: Overall number of files ({:>,d}) is large\n".format(trunc((counts['file'][notinarc]+counts['file'][inarc]))) + \
		"    Impact:  Scan time could be long\n" + \
		"    Action:  Ignore folders or split project (scan sub-projects) - look for .bdignore in report file\n\n"

	#
	# Need to add check for nothing to scan (no supported scan files)
	if counts['src'][notinarc]+counts['src'][inarc]+counts['jar'][notinarc]+counts['jar'][inarc]+counts['other'][notinarc]+counts['other'][inarc] == 0:
		recs_msgs_dict['info'] += "- INFORMATION: No source, jar or other files found\n".format(trunc((counts['file'][notinarc]+sizes['file'][inarc]))) + \
		"    Impact:  Scan may not detect any OSS from files (dependencies only)\n" + \
		"    Action:  Check scan location is correct\n"

	if sizes['bin'][notinarc]+sizes['bin'][inarc] > 20000000:
		recs_msgs_dict['imp'] += "- IMPORTANT: Large amount of data ({:>,d} MB) in {} binary files found\n".format(trunc((sizes['bin'][notinarc]+sizes['bin'][inarc])/1000000), len(bin_list)) + \
		"    Impact:  Binary files not analysed by standard scan, will impact Capacity license usage\n" + \
		"    Action:  Remove files or ignore folders (using .bdignore), also consider zipping\n" + \
		"             files and using Binary scan (Specify -f option to add list of large binary\n" + \
		"             files to the report file, and use the --detect.binary.scan.file.path=binary_files.zip option)\n\n"
		cli_msgs_dict['scan'] += "--detect.binary.scan.file.path=binary_files.zip\n" + \
		"    (To scan binary files within the project; zip files first - see list of binary\n" + \
		"    files in report file; binary scan license required)\n"
		if f:
			f.write("\nLARGE BINARY FILES:\n(Consider zipping these files and then running Detect with --detect.binary.scan.file.path=binary_files.zip option - subject to license available)\n")
			for bin in bin_large_list:
				f.write("    {}\n".format(bin))
			f.write("\n")

	if size_dirdups > 20000000:
		recs_msgs_dict['imp'] += "- IMPORTANT: Large amount of data ({:,d} MB) in {:,d} duplicate folders\n".format(trunc(size_dirdups/1000000), len(dup_dir_dict)) + \
		"    Impact:  Scan capacity potentially utilised without detecting additional\n" + \
		"             components, will impact Capacity license usage\n" + \
		"    Action:  Remove or ignore duplicate folders (recommended .bdignore file contents in report file)\n\n"
		for apath, bpath in dup_dir_dict.items():
			if bpath.find("##") < 0:
				bdignore += bpath + "\n"

	if size_dups > 20000000:
		recs_msgs_dict['imp'] += "- IMPORTANT: Large amount of data ({:,d} MB) in {:,d} duplicate files\n".format(trunc(size_dups/1000000), len(dup_large_dict)) + \
		"    Impact:  Scan capacity potentially utilised without detecting additional\n" + \
		"             components, will impact Capacity license usage\n" + \
		"    Action:  Remove duplicate files or ignore folders (recommended .bdignore file contents in report file)\n\n"
		#for apath, bpath in dup_large_dict.items():
		#	if dup_dir_dict.get(os.path.dirname(apath)) == None and dup_dir_dict.get(os.path.dirname(bpath)) == None:
		#		print("    {}".format(bpath))
		#print("")

	if counts['lic'][notinarc] > 10:
		recs_msgs_dict['info'] += "- INFORMATION: License or notices files found\n" + \
		"    Impact:  Local license text may need to be scanned\n" + \
		"    Action:  Add options --detect.blackduck.signature.scanner.license.search=true\n" + \
		"             and optionally --detect.blackduck.signature.scanner.upload.source.mode=true\n\n"
		cli_msgs_dict['lic'] += "--detect.blackduck.signature.scanner.license.search=true\n" + \
		"    (To perform client-side scanning for license files and references)\n"
		if cli_msgs_dict['lic'].find("upload.source.mode") < 0:
			cli_msgs_dict['lic'] += "--detect.blackduck.signature.scanner.upload.source.mode=true\n" + \
			"    (CAUTION - will upload local source files)\n"

	if counts['src'][notinarc]+counts['src'][inarc] > 10:
		recs_msgs_dict['info'] += "- INFORMATION: Source files found for which Snippet analysis supported\n" + \
		"    Impact:  Snippet analysis can discover copied OSS source files and functions\n" + \
		"    Action:  Add options --detect.blackduck.signature.scanner.snippet.matching=SNIPPET_MATCHING\n\n"
		cli_msgs_dict['lic'] += "--detect.blackduck.signature.scanner.snippet.matching=SNIPPET_MATCHING\n" + \
		"    (To search for copied OSS source files and functions within source files)\n"
		if cli_msgs_dict['lic'].find("upload.source.mode") < 0:
			cli_msgs_dict['lic'] += "--detect.blackduck.signature.scanner.upload.source.mode=true\n" + \
			"    (CAUTION - will upload local source files)\n"

	check_singlefiles(f)
	print(" Done")
	print("")

def detector_process(folder, f):
	import shutil

	global rep

	print("- Processing Dependency Scan .....", end="", flush=True)

	if f:
		f.write("PROJECT FILES FOUND:\n")

	det_depth1 = 0
	det_other = 0
	cmds_missing1 = ""
	cmds_missingother = ""
	cmds_missing_list = []
	det_max_depth = 0
	det_min_depth = 100
	det_in_arc = 0
	if len(det_dict) > 0:
		for detpath, depth in det_dict.items():
			command_exists = False
			if detpath.find("##") > 0:
				# in archive
				det_in_arc += 1
			else:
				if depth == 1:
					det_depth1 += 1
				elif depth > 1:
					det_other += 1
				if depth > det_max_depth:
					det_max_depth = depth
				if depth < det_min_depth:
					det_min_depth = depth
				fname = os.path.basename(detpath)
				exes = ""
				if fname in detectors_file_dict.keys():
					exes = detectors_file_dict[fname]
				elif os.path.splitext(fname)[1] in detectors_ext_dict.keys():
					exes = detectors_ext_dict[os.path.splitext(fname)[1]]
				missing_cmds = ""
				for exe in exes:
					if exe not in detectors_list:
						detectors_list.append(exe)
						if platform.system() != "Linux" and exe in linux_only_detectors:
							if depth == 1:
								recs_msgs_dict['crit'] += "- CRITICAL: Package manager '{}' requires scanning on a Linux platform\n".format(exe) + \
								"    Impact:  Scan will fail\n" + \
								"    Action:  Re-run Detect scan on Linux\n\n"
							else:
								recs_msgs_dict['imp'] += "- IMPORTANT: Package manager '{}' requires scanning on a Linux platform\n".format(exe) + \
								"    Impact:  Scan may fail if detector depth changed from default value 0\n" + \
								"    Action:  Re-run Detect scan on Linux\n\n"
					if shutil.which(exe) is not None:
						command_exists = True
					else:
						if exe not in cmds_missing_list:
							cmds_missing_list.append(exe)
							if missing_cmds:
								missing_cmds += " OR " + exe
							else:
								missing_cmds = exe
				if f:
					f.write("{}\n".format(detpath))

				if not command_exists and missing_cmds:
					if missing_cmds.find(" OR ") > 0:
						missing_cmds = "(" + missing_cmds + ")"
					if depth == 1:
						if cmds_missing1:
							cmds_missing1 += " AND " + missing_cmds
						else:
							cmds_missing1 = missing_cmds
					else:
						if cmds_missingother:
							cmds_missingother += " AND " + missing_cmds
						else:
							cmds_missingother = missing_cmds

		rep = "\nPACKAGE MANAGER CONFIG FILES:\n" + \
		"- In invocation folder:   {}\n".format(det_depth1) + \
		"- In sub-folders:         {}\n".format(det_other) + \
		"- In archives:            {}\n".format(det_in_arc) + \
		"- Minimum folder depth:   {}\n".format(det_min_depth) + \
		"- Maximum folder depth:   {}\n".format(det_max_depth) + \
		"---------------------------------\n" + \
		"- Total discovered:       {}\n\n".format(len(det_dict)) + \
		"Config files for the following Package Managers found: {}\n".format(', '.join(detectors_list))

	if det_depth1 == 0 and det_other > 0:
		recs_msgs_dict['imp'] += "- IMPORTANT: No package manager files found in invocation folder but do exist in sub-folders\n" + \
		"    Impact:  Dependency scan will not be run\n" + \
		"    Action:  Specify --detect.detector.depth={} (although depth could be up to {})\n" + \
		"             or scan sub-folders seperately.\n".format(det_min_depth, det_max_depth)
		if cli_msgs_dict['scan'].find("detector.depth") < 0:
			cli_msgs_dict['scan'] += "--detect.detector.depth={}\n".format(det_min_depth) + \
			"    (To find package manager files within sub-folders; note depth {} would find\n".format(det_max_depth) + \
			"    all PM files in sub-folders but higher level projects may already include these)\n"

	if det_depth1 == 0 and det_other == 0:
		recs_msgs_dict['info'] += "- INFORMATION: No package manager files found in project at all\n" + \
		"    Impact:  No dependency scan will be performed\n" + \
		"    Action:  This may be expected, but ensure you are scanning the correct location\n\n"

	if cmds_missing1:
		recs_msgs_dict['crit'] += "- CRITICAL: Package manager programs ({}) missing for package files in invocation folder\n".format(cmds_missing1) + \
		"    Impact:  Scan will fail\n" + \
		"    Action:  Either install package manager programs or\n" + \
		"             consider specifying --detect.detector.buildless=true\n\n"
		cli_msgs_dict['reqd'] += "--detect.detector.buildless=true\n" + \
		"    (OR specify --detect.XXXX.path=<LOCATION> where XXX is package manager\n" + \
		"    OR install package managers '{}')\n".format(cmds_missing1)

	if cmds_missingother:
		recs_msgs_dict['imp'] += "- IMPORTANT: Package manager programs ({}) missing for package files in sub-folders\n".format(cmds_missingother) + \
		"    Impact:  The scan will fail if the scan depth is modified from the default\n" + \
		"    Action:  Install package manager programs\n" + \
		"             (OR specify --detect.XXXX.path=<LOCATION> where XXX is package manager\n" + \
		"             OR specify --detect.detector.buildless=true)\n\n"
		if cli_msgs_dict['scan'].find("detector.buildless") < 0:
			cli_msgs_dict['scan'] += "--detect.detector.buildless=true\n" + \
			"    (OR install package managers '{}'\n".format(cmds_missingother) + \
			"    (OR use --detect.XXXX.path=<LOCATION> where XXX is package manager)\n"

	if counts['det'][inarc] > 0:
		recs_msgs_dict['imp'] += "- IMPORTANT: Package manager files found in archives\n" + \
		"    Impact:  Dependency scan not performed for projects in archives\n" + \
		"    Action:  Extract zip archives and rescan\n\n"

	for cmd in detectors_list:
		if cmd in detector_cli_options_dict.keys():
			cli_msgs_dict['dep'] += " For {}:\n".format(cmd) + detector_cli_options_dict[cmd]
		if cmd in detector_cli_required_dict.keys():
			cli_msgs_dict['crit'] += " For {}:\n".format(cmd) + detector_cli_required_dict[cmd]

	print(" Done")

	return

def output_recs(critical_only, f):
	global messages
	global bdignore

	if f:
		f.write(messages + "\n")

	print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nRECOMMENDATIONS:\n")
	if f:
		f.write("\nRECOMMENDATIONS:\n")

	if recs_msgs_dict['crit']:
		print(recs_msgs_dict['crit'])
		if f:
			f.write(recs_msgs_dict['crit'] + "\n")

	if recs_msgs_dict['imp']:
		if not critical_only:
			if recs_msgs_dict['crit']:
				print("-----------------------------------------------------------------------------------------------------")
			print(recs_msgs_dict['imp'])
		if f:
			if recs_msgs_dict['crit']:
				f.write("-----------------------------------------------------------------------------------------------------\n")
			f.write(recs_msgs_dict['imp'] + "\n")

	if recs_msgs_dict['info']:
		if not critical_only:
			if recs_msgs_dict['crit'] or recs_msgs_dict['imp']:
				print("-----------------------------------------------------------------------------------------------------")
			print(recs_msgs_dict['info'])
		if f:
			if recs_msgs_dict['crit'] or recs_msgs_dict['imp']:
				f.write("-----------------------------------------------------------------------------------------------------\n")
			f.write(recs_msgs_dict['info'] + "\n")

	if (not recs_msgs_dict['crit'] and not recs_msgs_dict['imp'] and not recs_msgs_dict['info']):
		print("- None\n")
		if f:
			f.write("None\n")

	if (critical_only and not recs_msgs_dict['crit']):
		print("- No Critical Recommendations\n")

	if bdignore and f:
		f.write("\nRECOMMENDED .bdignore FILE CONTENTS:\n(.bdignore file must be created in invocation or project folder)\n\n" + bdignore + "\n")

def check_prereqs():
	import subprocess
	import shutil

	global rep

	global messages

	# Check java
	try:
		if shutil.which("java") is None:
			recs_msgs_dict['crit'] += "- CRITICAL: Java is not installed or on the PATH\n" + \
			"    Impact:  Detect program will fail\n" + \
			"    Action:  Install OpenJDK 1.8 or 1.11\n\n"
# 			if cli_msgs_dict['reqd'].find("detect.java.path") < 0:
# 				cli_msgs_dict['reqd'] += ""    --detect.java.path=<PATH_TO_JAVA>\n" + \
# 				"    (If Java installed, specify path to java executable if not on PATH)\n"
		else:
			try:
				javaoutput = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)
				#javaoutput = 'openjdk version "13.0.1" 2019-10-15'
				#javaoutput = 'java version "1.8.0_181"'
				crit = True
				if javaoutput:
					line0 = javaoutput.decode("utf-8").splitlines()[0]
					prog = line0.split(" ")[0].lower()
					if prog:
						version_string = line0.split('"')[1]
						if version_string:
							major, minor, _ = version_string.split('.')
							if prog == "openjdk":
								crit = False
								if major == "8" or major == "11":
									rec = "none"
								else:
									recs_msgs_dict['imp'] += "- IMPORTANT: OpenJDK version {} is not documented as supported by Detect\n".format(version_string) + \
									"    Impact:  Scan may fail\n" + \
									"    Action:  Check that Detect operates correctly\n\n"
							elif prog == "java":
								crit = False
								if major == "1" and (minor == "8" or minor == "11"):
									rec = "none"
								else:
									recs_msgs_dict['imp'] += "- IMPORTANT: Java version {} is not documented as supported by Detect\n".format(version_string) + \
									"    Impact:  Scan may fail\n" + \
									"    Action:  Check that Detect operates correctly\n\n"
			except:
				crit = True

			if crit:
				recs_msgs_dict['crit'] += "- CRITICAL: Java program version cannot be determined\n" + \
				"    Impact:  Scan may fail\n" + \
				"    Action:  Check Java or OpenJDK version 1.8 or 1.11 is installed\n\n"
# 				if cli_msgs_dict['reqd'].find("detect.java.path") < 0:
# 					cli_msgs_dict['reqd'] += "--detect.java.path=<PATH_TO_JAVA>\n" + \
# 					"    (If Java installed, specify path to java executable if not on PATH)\n"

	except:
		recs_msgs_dict['crit'] += "- CRITICAL: Java is not installed or on the PATH\n" + \
		"    Impact:  Detect program will fail\n" + \
		"    Action:  Install OpenJDK 1.8 or 1.11\n\n"
# 		if cli_msgs_dict['reqd'].find("detect.java.path") < 0:
# 			cli_msgs_dict['reqd'] += "--detect.java.path=<PATH_TO_JAVA>\n" + \
# 			"    (If Java installed, specify path to java executable if not on PATH)\n"

	if platform.system() == "Linux" or platform.system() == "Darwin":
		# check for bash and curl
		if shutil.which("bash") is None:
			recs_msgs_dict['crit'] += "- CRITICAL: Bash is not installed or on the PATH\n" + \
			"    Impact:  Detect program will fail\n" + \
			"    Action:  Install Bash or add to PATH\n\n"

	if shutil.which("curl") is None:
		recs_msgs_dict['crit'] += "- CRITICAL: Curl is not installed or on the PATH\n" + \
		"    Impact:  Detect program will fail\n" + \
		"    Action:  Install Curl or add to PATH\n\n"
	else:
		if not check_connection("https://detect.synopsys.com"):
			recs_msgs_dict['crit'] += "- CRITICAL: No connection to https://detect.synopsys.com\n" + \
			"    Impact:  Detect wrapper script cannot be downloaded, online scan not possible\n" + \
			"    Action:  Download Detect manually and run offline (see docs)\n\n"

		if not check_connection("https://sig-repo.synopsys.com"):
			recs_msgs_dict['crit'] += "- CRITICAL: No connection to https://sig-repo.synopsys.com\n" + \
			"    Impact:  Detect jar cannot be downloaded; online scan cannot be performed\n" + \
			"    Action:  Download Detect manually and run offline (see docs)\n\n"

def check_connection(url):
	import subprocess

	try:
		output = subprocess.check_output(['curl', '-s', '-m', '5', url], stderr=subprocess.STDOUT)
		return True
	except:
		return False

def check_docker_prereqs():
	import shutil
	import subprocess

	if platform.system() != "Linux" and platform.system() != "Darwin":
		recs_msgs_dict['crit'] += "- CRITICAL: Docker image scanning only supported on Linux or MacOS\n" + \
		"    Impact:  Docker image scan will fail\n" + \
		"    Action:  Perform scan Docker on Linux or MacOS\n\n"
	else:
		if shutil.which("docker") is None:
			recs_msgs_dict['crit'] += "- CRITICAL: Docker not installed - required for Docker image scanning\n" + \
			"    Impact:  Docker image scan will fail\n" + \
			"    Action:  Install docker\n\n"
		else:
			try:
				output = subprocess.check_output(['docker', 'run', 'hello-world'], stderr=subprocess.STDOUT)
			except:
				recs_msgs_dict['crit'] += "- CRITICAL: Docker could not be started\n" + \
				"    Impact:  Detect image scan will fail (docker inspector cannot be started)\n" + \
				"    Action:  Check docker permissions OR not running within container\n\n"

		if shutil.which("curl") is not None:
			if not check_connection("https://blackducksoftware.github.io"):
				recs_msgs_dict['crit'] += "- CRITICAL: No connection to https://blackducksoftware.github.io\n" + \
				"    Impact:  Detect docker inspector cannot be downloaded; online scan cannot be performed\n" + \
				"    Action:  Download docker inspector manually and run offline (see docs)\n\n"

def output_cli(critical_only, report, f):
	global bdignore

	output = "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nDETECT CLI:\n\n"
	if recs_msgs_dict['crit']:
		output += "Note that scan will probably fail - see CRITICAL recommendations above\n\n"

	output += "    MINIMUM REQUIRED OPTIONS:\n"
	output += re.sub(r"^", "    ", cli_msgs_dict['reqd'], flags=re.MULTILINE)

	print(output)
	if bdignore:
		if report:
			print("        (Note that '.bdignore' exclude file is recommended - see the report file '{}' or use '-o' option\n" + \
			"        to create '.bdignore' file)\n".format(report))
		else:
			print("        (Note that '.bdignore' exclude file is recommended - create a report file using '-r repfile' to\n" + \
			"        see recommended folders to exclude or use '-o' option to create '.bdignore' file)\n")
	if f:
		f.write(output + "\n")

	output = ""
	if cli_msgs_dict['scan'] != '':
		output += "\nOPTIONS TO IMPROVE SCAN COVERAGE:\n" + cli_msgs_dict['scan'] + "\n"

	if cli_msgs_dict['size'] != '':
		output += "\nOPTIONS TO REDUCE SIGNATURE SCAN SIZE:\n" + cli_msgs_dict['size'] + "\n"

	if cli_msgs_dict['dep'] != '':
		output += "\nOPTIONS TO OPTIMIZE DEPENDENCY SCAN:\n" + cli_msgs_dict['dep'] + "\n"

	if cli_msgs_dict['lic'] != '':
		output += "\nOPTIONS TO IMPROVE LICENSE COMPLIANCE ANALYSIS:\n" + cli_msgs_dict['lic'] + "\n"

	if cli_msgs_dict['proj'] != '':
		output += "\nPROJECT OPTIONS:\n" + cli_msgs_dict['proj'] + "\n"

	if cli_msgs_dict['rep'] != '':
		output += "\nREPORTING OPTIONS:\n" + cli_msgs_dict['rep'] + "\n"

	output = re.sub(r"^", "    ", output, flags=re.MULTILINE)

	if not critical_only:
		print(output)
	if f:
		f.write(output + "\n")

	if f:
		print("INFO: Output report file '{}' created".format(report))
	else:
		print("INFO: Use '-r repfile' to produce report file with more information")

def output_config(projdir):
	global bdignore

	bdignore_file = os.path.join(projdir, ".bdignore")
	if not os.path.exists(bdignore_file):
		try:
			b = open(bdignore_file, "a")
			b.write(bdignore)
			b.close()
			print("INFO: '.bdignore' file created in project folder")
		except Exception as e:
			print('ERROR: Unable to open .bdignore file \n' + str(e))
	else:
		print("INFO: '.bdignore' file already exists - not updated")

	config_file = os.path.join(projdir, "application-project.yml")
	if not os.path.exists(config_file):
		config = "#\n# EXAMPLE PROJECT CONFIG FILE\n" + \
		"# Uncomment and update required options\n#\n#\n" + \
		"# MINIMUM REQUIRED OPTIONS:\n#\n" + cli_msgs_dict['reqd'] + "\n" + \
		"# OPTIONS TO IMPROVE SCAN COVERAGE:\n#\n" + cli_msgs_dict['scan'] + "\n" + \
		"# OPTIONS TO REDUCE SIGNATURE SCAN SIZE:\n#\n" + cli_msgs_dict['size'] + "\n" + \
		"# OPTIONS TO CONFIGURE DEPENDENCY SCAN:\n#\n" + cli_msgs_dict['dep'] + "\n" + \
		"# OPTIONS TO IMPROVE LICENSE COMPLIANCE ANALYSIS:\n#\n" + cli_msgs_dict['lic'] + "\n" + \
		"# PROJECT OPTIONS:\n#\n" + cli_msgs_dict['proj'] + "\n" + \
		"# REPORTING OPTIONS:\n#\n" + cli_msgs_dict['rep'] + "\n"

		config = re.sub("=", ": ", config)
		config = re.sub(r"\n ", r"\n#", config, flags=re.S)
		config = re.sub(r"\n--", r"\n#", config, flags=re.S)
		try:
			c = open(config_file, "a")
			c.write(config)
			c.close()
			print("INFO: Config file 'application-project.yml' file written to project folder (Edit to uncomment options)\n" + \
			"      - Use '--spring.profiles.active=project' to specify this configuration")
		except Exception as e:
			print('ERROR: Unable to create project config file ' + str(e))
	else:
		print("INFO: Project config file 'application-project.yml' already exists - not updated")

def check_input_options(prompt, accepted_values):
	value = input(prompt)
	if value == "":
		return(0)
	ret = value[0].lower()
	if ret == "q":
		raise Exception("quit")
	ind = 0
	for val in accepted_values:
		if ret == val[0].lower():
			return(ind)
		ind += 1
	raise Exception("quit")

def check_input_yn(prompt, default):
	value = input(prompt)
	if value == "":
		return(default)
	ret = value[0].lower()
	if ret == "q":
		raise Exception("quit")
		print("GOT HERE")
	if ret == "y":
		return(True)
	elif ret == "n":
		return(False)
	raise Exception("quit")

def interactive():
	report = ""

	try:
		folder = input("Enter project folder to scan (default current folder '{}'):".format(os.getcwd()))
	except:
		print("Exiting")
		raise("quit")
		return("", "", False, False, "", False)
	if folder == "":
		folder = "."
	elif not os.path.isdir(folder):
		print("Scan location '{}' does not exist\nExiting".format(folder))
		raise("quit")
		return("", "", False, False, "", False)
	try:
		scan_type = check_input_options("Types of scan to check? (B)oth, (d)ependency or (s)ignature] [b]:", ['b','d','s'])
		docker_bool = check_input_yn("Docker scan check? (y/n) [n]:", False)
		critical_bool = check_input_yn("Critical recommendations only? (y/n) [n]:", False)
		report_bool = check_input_yn("Create output report file? (y/n) [y]:", True)
		if report_bool:
			report_file = input("Report file name:")
		config_bool = check_input_yn("Create .bdignore & application-project.yml file? (y/n) [n]:", False)
	except:
		print("Exiting")
		raise("quit")
		return("", "", False, False, "", False)
	return(folder, scan_type, docker_bool, critical_bool, report, config_bool)

parser = argparse.ArgumentParser(description='Check prerequisites for Detect, scan folders, provide recommendations and example CLI options', prog='detect_advisor')

parser.add_argument("scanfolder", nargs="?", help="Project folder to analyse", default="")

parser.add_argument("-r", "--report", help="Output report file (must not exist already)")
parser.add_argument("-d", "--detector_only", help="Check for detector files and prerequisites only",action='store_true')
parser.add_argument("-s", "--signature_only", help="Check for files and folders for signature scan only",action='store_true')
parser.add_argument("-c", "--critical_only", help="Only show critical issues which will causes detect to fail",action='store_true')
#parser.add_argument("-f", "--full", help="Output full information to report file if specified",action='store_true')
parser.add_argument("-o", "--output_config", help="Create .yml config and .bdignore file in project folder",action='store_true')
parser.add_argument("-D", "--docker", help="Check docker prerequisites",action='store_true')
parser.add_argument("--docker_only", help="Only check docker prerequisites",action='store_true')

args = parser.parse_args()

if args.scanfolder == "":
	try:
		args.scanfolder, scan_type, args.docker, args.critical_only, args.report, args.output_config = interactive()
		if scan_type == "d":
			args.detector_only = True
		elif scan_type == "s":
			args.signature_only = True
	except:
		exit(1)

if not os.path.isdir(args.scanfolder):
	print("Scan location '{}' does not exist\nExiting".format(args.scanfolder))
	exit(1)

if args.report and os.path.exists(args.report):
	print("Report file '{}' already exists\nExiting".format(args.report))
	exit(2)

rep = ""
bdignore = ""
cli_msgs_dict['reqd'] += "--detect.source.path='{}'\n".format(os.path.abspath(args.scanfolder))

print("\nDETECT ADVISOR v{} - for use with Synopsys Detect v{} or later\n".format(advisor_version, detect_version))

print("PROCESSING:")

if os.path.isabs(args.scanfolder):
	print("Working on project folder '{}'\n".format(args.scanfolder))
else:
	print("Working on project folder '{}' (Absolute path '{}')\n".format(args.scanfolder, os.path.abspath(args.scanfolder)))

print("- Reading hierarchy          .....", end="", flush=True)
process_dir(args.scanfolder, 0)
print(" Done")

if args.report:
	try:
		f = open(args.report, "a")
	except Exception as e:
		print('ERROR: Unable to create output report file \n' + str(e))
		exit(3)
else:
	f = None

if not args.signature_only and not args.docker_only:
#	if args.full:
	if True:
		detector_process(args.scanfolder, f)
	else:
		detector_process(args.scanfolder, None)
if args.signature_only:
	cli_msgs_dict['reqd'] += "--detect.tools=SIGNATURE_SCAN\n"

if not args.detector_only and not args.docker_only:
#	if args.full:
	if True:
			signature_process(args.scanfolder, f)
	else:
		signature_process(args.scanfolder, None)
if args.detector_only:
	cli_msgs_dict['reqd'] += "--detect.tools=DETECTOR\n"

print_summary(args.critical_only, f)

check_prereqs()

if args.docker or args.docker_only:
	check_docker_prereqs()
if args.docker_only:
	cli_msgs_dict['reqd'] += "--detect.tools=DOCKER\n"

output_recs(args.critical_only, f)

output_cli(args.critical_only, args.report, f)

if args.output_config:
	output_config(args.scanfolder)

print("")
if f:
	f.write("\n")
	f.close()
